{"version":3,"sources":["../../src/Service/CrawlService.js"],"names":["CrawlService","Service","start","urlsRepository","JsonUrlsRepository","getPathJsonUrlsFile","htmlRepository","HtmlRepository","getProjectPath","crawlerRepository","CrawlerRepository","args","option","crawlStatesRepository","SqliteCrawlStatesRepository","initUrl","getSingleUrl","isSingle","initUrlsPool","findAllUrls","progress","emitProgress","html","url","save","then","urls","emitComplete"],"mappings":"6FACA,0DACA,mEACA,4FACA,0FACA,oFAEA,8G,kFAEA;;GAGe,KAAMA,CAAAA,YAAN,QAA2BC,iBAAQ,CAC9C;;OAGAC,KAAK,EAAG,CACJ,GAAIC,CAAAA,cAAc,CAAG,GAAIC,4BAAJ,CAAuB,KAAKC,mBAAL,EAAvB,CAArB,CACA,GAAIC,CAAAA,cAAc,CAAG,GAAIC,wBAAJ,CAAmB,KAAKC,cAAL,EAAnB,CAArB,CACA,GAAIC,CAAAA,iBAAiB,CAAG,GAAIC,2BAAJ,CAAsB,KAAKC,IAA3B,CAAiC,KAAKC,MAAtC,CAAxB,CAEA,GAAIC,CAAAA,qBAAqB,CAAG,GAAIC,qCAAJ,CACxB,KAAKN,cAAL,EADwB,CAA5B,CAGAC,iBAAiB,CAACI,qBAAlB,CAA0CA,qBAA1C,CAEA,KAAME,CAAAA,OAAO,CAAG,KAAKJ,IAAL,CAAUK,YAAV,EAAhB,CACA,GAAI,KAAKL,IAAL,CAAUM,QAAV,IAAwBF,OAAO,EAAI,IAAvC,CAA6C,CACzCF,qBAAqB,CAACK,YAAtB,CAAmC,CAACH,OAAD,CAAnC,CACH,CACDN,iBAAiB,CACZU,WADL,CACkBC,QAAD,EAAwB,CACjC,KAAKC,YAAL,CAAkBD,QAAlB,EACA,GAAI,KAAKT,IAAL,CAAUW,IAAV,EAAkBF,QAAQ,CAACG,GAAT,EAAgB,IAAlC,EAA0CH,QAAQ,CAACE,IAAT,EAAiB,IAA/D,CAAqE,CACjEhB,cAAc,CAACkB,IAAf,CAAoBJ,QAAQ,CAACG,GAA7B,CAAkCH,QAAQ,CAACE,IAA3C,EAAiDG,IAAjD,EACH,CACJ,CANL,EAOKA,IAPL,CAOUC,IAAI,EAAI,CACVvB,cAAc,CAACqB,IAAf,CAAoBE,IAApB,EAA0BD,IAA1B,CAA+B,IAAM,KAAKE,YAAL,EAArC,CACH,CATL,CAUH,CA5B6C,C","sourcesContent":["// @flow\nimport Service from \"./Service\";\nimport Progress from \"../Model/Progress\";\nimport JsonUrlsRepository from \"../Repository/JsonUrlsRepository\";\nimport CrawlerRepository from \"../Repository/CrawlerRepository\";\nimport HtmlRepository from \"../Repository/HtmlRepository\";\n\nimport SqliteCrawlStatesRepository from \"../Repository/SqliteCrawlStatesRepository\";\n\n/**\n * This service will extract all the urls from a domain by crawling a site.\n */\nexport default class CrawlService extends Service {\n    /**\n     * Start the crawl service to extract the domain urls.\n     */\n    start() {\n        let urlsRepository = new JsonUrlsRepository(this.getPathJsonUrlsFile());\n        let htmlRepository = new HtmlRepository(this.getProjectPath());\n        let crawlerRepository = new CrawlerRepository(this.args, this.option);\n\n        let crawlStatesRepository = new SqliteCrawlStatesRepository(\n            this.getProjectPath()\n        );\n        crawlerRepository.crawlStatesRepository = crawlStatesRepository;\n\n        const initUrl = this.args.getSingleUrl();\n        if (this.args.isSingle() && initUrl != null) {\n            crawlStatesRepository.initUrlsPool([initUrl]);\n        }\n        crawlerRepository\n            .findAllUrls((progress: Progress) => {\n                this.emitProgress(progress);\n                if (this.args.html && progress.url != null && progress.html != null) {\n                    htmlRepository.save(progress.url, progress.html).then();\n                }\n            })\n            .then(urls => {\n                urlsRepository.save(urls).then(() => this.emitComplete());\n            });\n    }\n}\n"],"file":"CrawlService.js"}