{"version":3,"sources":["../../src/Repository/CrawlerRepository.js"],"names":["CrawlerRepository","constructor","args","option","initialUrl","domain","findAllUrls","progress","crawlStatesRepository","urlsPoolSize","addPoolUrl","Promise","resolve","crawlNextUrl","url","cleanUrl","popPoolUrl","JSDOM","fromURL","then","dom","newUrl","Url","addUrl","innerHTML","window","document","documentElement","urlsSize","Progress","isSingle","links","querySelectorAll","length","link","foundUrl","href","isFreshUrl","catch","urls","findAttemptedUrls","isInDomain","isNotExclusion","isNotRecursive","isNotDocument","path","UrlParser","parse","urlsParsed","exclusion","index","exclusions","startsWith","endsWith","uri","replace","split","entries","splice","entry","found","filter","e"],"mappings":";;;;;;;AACA;;AACA;;AAEA;;AACA;;AACA;;AACA;;AAEA;;;;AAEA;;;AAGe,MAAMA,iBAAN,CAAwB;AACrC;;;;AAIA;;;;AAIA;;;;AAIA;;;;AAOA;;;AAGAC,EAAAA,WAAW,CAACC,IAAD,EAAaC,MAAb,EAA6B;AACtC,SAAKC,UAAL,GAAmB,UAASF,IAAI,CAACG,MAAO,GAAxC;AACA,SAAKH,IAAL,GAAYA,IAAZ;AACA,SAAKC,MAAL,GAAcA,MAAd;AACD;AAED;;;;;AAGAG,EAAAA,WAAW,CAACC,QAAD,EAA6C;AACtD,SAAKA,QAAL,GAAgBA,QAAhB;;AACA,QAAI,KAAKC,qBAAL,CAA2BC,YAA3B,OAA8C,CAAlD,EAAqD;AACnD,WAAKD,qBAAL,CAA2BE,UAA3B,CAAsC,KAAKN,UAA3C;AACD;;AACD,WAAO,IAAIO,OAAJ,CAAmBC,OAAO,IAAI;AACnC,WAAKA,OAAL,GAAeA,OAAf;AACA,WAAKC,YAAL;AACD,KAHM,CAAP;AAID;AAED;;;;;;AAIAA,EAAAA,YAAY,GAAG;AACb,UAAMJ,YAAY,GAAG,KAAKD,qBAAL,CAA2BC,YAA3B,EAArB;;AACA,QAAIA,YAAY,KAAK,CAArB,EAAwB;AACtB,aAAO,KAAKG,OAAL,CAAa,KAAKJ,qBAAL,CAA2BF,WAA3B,EAAb,CAAP;AACD;;AACD,QAAIQ,GAAG,GAAGd,iBAAiB,CAACe,QAAlB,CACR,KAAKP,qBAAL,CAA2BQ,UAA3B,EADQ,CAAV;;AAGAC,iBAAMC,OAAN,CAAcJ,GAAd,EACGK,IADH,CACQC,GAAG,IAAI;AACX,YAAMC,MAAM,GAAG,IAAIC,YAAJ,CAAQR,GAAR,CAAf;AACA,WAAKN,qBAAL,CAA2Be,MAA3B,CAAkCF,MAAlC;AAFW,YAGHG,SAHG,GAGWJ,GAAG,CAACK,MAAJ,CAAWC,QAAX,CAAoBC,eAH/B,CAGHH,SAHG;AAIX,YAAMI,QAAQ,GAAG,KAAKpB,qBAAL,CAA2BoB,QAA3B,EAAjB;AACA,WAAKrB,QAAL,CACE,IAAIsB,iBAAJ,CAAaR,MAAb,EAAqBG,SAArB,EAAgCI,QAAhC,EAA0CnB,YAAY,GAAG,CAAzD,CADF;;AAGA,UAAI,KAAKP,IAAL,CAAU4B,QAAV,EAAJ,EAA0B;AACxB,eAAO,KAAKlB,OAAL,CAAa,KAAKJ,qBAAL,CAA2BF,WAA3B,EAAb,CAAP;AACD,OAFD,MAEO;AACL,cAAMyB,KAAK,GAAGX,GAAG,CAACK,MAAJ,CAAWC,QAAX,CAAoBM,gBAApB,CAAqC,GAArC,CAAd;AACA,cAAMC,MAAM,GAAGF,KAAK,CAACE,MAArB;;AACA,aAAK,IAAIC,IAAT,IAAiBH,KAAjB,EAAwB;AACtB,cAAII,QAAQ,GAAGnC,iBAAiB,CAACe,QAAlB,CAA2BmB,IAAI,CAACE,IAAhC,CAAf;;AACA,cAAI,KAAKC,UAAL,CAAgBF,QAAhB,CAAJ,EAA+B;AAC7B,iBAAK3B,qBAAL,CAA2BE,UAA3B,CAAsCyB,QAAtC;AACD;AACF;;AACD,aAAKtB,YAAL;AACD;AACF,KAtBH,EAuBGyB,KAvBH,CAuBS,MAAM;AACX,WAAKzB,YAAL;AACD,KAzBH;AA0BD;AAED;;;;;;AAIAwB,EAAAA,UAAU,CAACvB,GAAD,EAAc;AACtB,UAAMyB,IAAI,GAAG,KAAK/B,qBAAL,CAA2BgC,iBAA3B,CAA6C1B,GAA7C,CAAb;AACA,WACEyB,IAAI,KAAK,CAAT,IACA,KAAKE,UAAL,CAAgB3B,GAAhB,CADA,IAEA,KAAK4B,cAAL,CAAoB5B,GAApB,CAFA,IAGAd,iBAAiB,CAAC2C,cAAlB,CAAiC7B,GAAjC,CAHA,IAIAd,iBAAiB,CAAC4C,aAAlB,CAAgC9B,GAAhC,CALF;AAOD;AAED;;;;;AAGA4B,EAAAA,cAAc,CAAC5B,GAAD,EAAuB;AACnC,QAAI+B,IAAY,GAAIC,aAAUC,KAAV,CAAgBjC,GAAhB,CAAD,CAA4BkC,UAA/C;;AACA,SAAK,IAAIC,SAAT,IAAsB,KAAK9C,MAAL,CAAY+C,KAAZ,CAAkBC,UAAxC,EAAoD;AAClD,UAAIN,IAAI,CAACO,UAAL,CAAgBH,SAAhB,CAAJ,EAAgC;AAC9B,eAAO,KAAP;AACD;AACF;;AACD,WAAO,IAAP;AACD;AAED;;;;;;AAIA,SAAOL,aAAP,CAAqB9B,GAArB,EAA2C;AACzC,WACE,CAACA,GAAG,CAACuC,QAAJ,CAAa,MAAb,CAAD,IACA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CADD,IAEA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CAFD,IAGA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CAHD,IAIA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CALH;AAOD;AAED;;;;;;AAIA,SAAOV,cAAP,CAAsB7B,GAAtB,EAA4C;AAC1C,QAAIwC,GAAG,GAAGxC,GAAG,CAACyC,OAAJ,CAAY,gBAAZ,EAA8B,EAA9B,EAAkCC,KAAlC,CAAwC,GAAxC,CAAV;AACA,UAAMC,OAAO,GAAGH,GAAG,CAACI,MAAJ,CAAW,CAAX,EAAcJ,GAAG,CAACrB,MAAlB,CAAhB;;AACA,SAAK,IAAI0B,KAAT,IAAkBF,OAAlB,EAA2B;AACzB,YAAMG,KAAK,GAAGH,OAAO,CAACI,MAAR,CAAeC,CAAC,IAAIA,CAAC,KAAKH,KAA1B,EAAiC1B,MAA/C;;AACA,UAAI2B,KAAK,GAAG,CAAZ,EAAe;AACb,eAAO,KAAP;AACD;AACF;;AACD,WAAO,IAAP;AACD;AAED;;;;;AAGAnB,EAAAA,UAAU,CAAC3B,GAAD,EAAuB;AAC/B,WAAOA,GAAG,CACPyC,OADI,CACI,gBADJ,EACsB,EADtB,EAEJH,UAFI,CAEO,OAAO,KAAKlD,IAAL,CAAUG,MAFxB,CAAP;AAGD;AAED;;;;;AAGA,SAAOU,QAAP,CAAgBD,GAAhB,EAAqC;AACnC,WAAOA,GAAG,CAAC0C,KAAJ,CAAU,GAAV,EAAe,CAAf,EAAkBA,KAAlB,CAAwB,GAAxB,EAA6B,CAA7B,CAAP;AACD;;AA3JoC","sourcesContent":["// @flow\r\nimport { JSDOM } from \"jsdom\";\r\nimport UrlParser from \"url\";\r\n\r\nimport Progress from \"../Model/Progress\";\r\nimport Url from \"../Model/Url\";\r\nimport Args from \"../Model/Args\";\r\nimport Option from \"../Model/Option\";\r\n\r\nimport SqliteCrawlStatesRepository from \"./SqliteCrawlStatesRepository\";\r\n\r\n/**\r\n * This crawler repository will use a domain name as a data-source and extract urls from it.\r\n */\r\nexport default class CrawlerRepository {\r\n  /**\r\n   * The initial sitemap url.\r\n   */\r\n  initialUrl: string;\r\n  /**\r\n   * Arguments passed to the app from the user.\r\n   */\r\n  args: Args;\r\n  /**\r\n   * Options loaded for the crawl.\r\n   */\r\n  option: Option;\r\n  /**\r\n   * Repository to access the crawl state.\r\n   */\r\n  crawlStatesRepository: SqliteCrawlStatesRepository;\r\n  progress: (Progress) => void;\r\n  resolve: any;\r\n\r\n  /**\r\n   * Build a sitemap repository\r\n   */\r\n  constructor(args: Args, option: Option) {\r\n    this.initialUrl = `http://${args.domain}/`;\r\n    this.args = args;\r\n    this.option = option;\r\n  }\r\n\r\n  /**\r\n   * Find all the urls on a site.\r\n   */\r\n  findAllUrls(progress: (Progress) => void): Promise<any> {\r\n    this.progress = progress;\r\n    if (this.crawlStatesRepository.urlsPoolSize() === 0) {\r\n      this.crawlStatesRepository.addPoolUrl(this.initialUrl);\r\n    }\r\n    return new Promise<Url[]>(resolve => {\r\n      this.resolve = resolve;\r\n      this.crawlNextUrl();\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Gets the page, if there are more pages it will add them to the list\r\n   * else, just adds the urls to the urls array.\r\n   */\r\n  crawlNextUrl() {\r\n    const urlsPoolSize = this.crawlStatesRepository.urlsPoolSize();\r\n    if (urlsPoolSize === 0) {\r\n      return this.resolve(this.crawlStatesRepository.findAllUrls());\r\n    }\r\n    let url = CrawlerRepository.cleanUrl(\r\n      this.crawlStatesRepository.popPoolUrl()\r\n    );\r\n    JSDOM.fromURL(url)\r\n      .then(dom => {\r\n        const newUrl = new Url(url);\r\n        this.crawlStatesRepository.addUrl(newUrl);\r\n        const { innerHTML } = dom.window.document.documentElement;\r\n        const urlsSize = this.crawlStatesRepository.urlsSize();\r\n        this.progress(\r\n          new Progress(newUrl, innerHTML, urlsSize, urlsPoolSize - 1)\r\n        );\r\n        if (this.args.isSingle()) {\r\n          return this.resolve(this.crawlStatesRepository.findAllUrls());\r\n        } else {\r\n          const links = dom.window.document.querySelectorAll(\"a\");\r\n          const length = links.length;\r\n          for (let link of links) {\r\n            let foundUrl = CrawlerRepository.cleanUrl(link.href);\r\n            if (this.isFreshUrl(foundUrl)) {\r\n              this.crawlStatesRepository.addPoolUrl(foundUrl);\r\n            }\r\n          }\r\n          this.crawlNextUrl();\r\n        }\r\n      })\r\n      .catch(() => {\r\n        this.crawlNextUrl();\r\n      });\r\n  }\r\n\r\n  /**\r\n   * Has the url been crawled before?\r\n   * @returns {boolean} true if the url has not been attempted.\r\n   */\r\n  isFreshUrl(url: string) {\r\n    const urls = this.crawlStatesRepository.findAttemptedUrls(url);\r\n    return (\r\n      urls === 0 &&\r\n      this.isInDomain(url) &&\r\n      this.isNotExclusion(url) &&\r\n      CrawlerRepository.isNotRecursive(url) &&\r\n      CrawlerRepository.isNotDocument(url)\r\n    );\r\n  }\r\n\r\n  /**\r\n   * Check to see if the url should be excluded.\r\n   */\r\n  isNotExclusion(url: string): boolean {\r\n    let path: string = (UrlParser.parse(url): any).urlsParsed;\r\n    for (let exclusion of this.option.index.exclusions) {\r\n      if (path.startsWith(exclusion)) {\r\n        return false;\r\n      }\r\n    }\r\n    return true;\r\n  }\r\n\r\n  /**\r\n   * This crawler only crawls html pages so make sure it is not something else.\r\n   * The next version will handle every document type.\r\n   */\r\n  static isNotDocument(url: string): boolean {\r\n    return (\r\n      !url.endsWith(\".pdf\") &&\r\n      !url.endsWith(\".jpg\") &&\r\n      !url.endsWith(\".png\") &&\r\n      !url.endsWith(\".gif\") &&\r\n      !url.endsWith(\".doc\")\r\n    );\r\n  }\r\n\r\n  /**\r\n   * Some sites I have crawled urls that are recursive and grow without a 404 being thrown. This\r\n   * method attempts to avoid those pages.\r\n   */\r\n  static isNotRecursive(url: string): boolean {\r\n    let uri = url.replace(/(https|http):/i, \"\").split(\"/\");\r\n    const entries = uri.splice(3, uri.length);\r\n    for (let entry of entries) {\r\n      const found = entries.filter(e => e === entry).length;\r\n      if (found > 1) {\r\n        return false;\r\n      }\r\n    }\r\n    return true;\r\n  }\r\n\r\n  /**\r\n   * The index will only crawl urls on the given domain.\r\n   */\r\n  isInDomain(url: string): boolean {\r\n    return url\r\n      .replace(/(https|http):/i, \"\")\r\n      .startsWith(\"//\" + this.args.domain);\r\n  }\r\n\r\n  /**\r\n   * Remove url params and hashes. They can lead to recursion.\r\n   */\r\n  static cleanUrl(url: string): string {\r\n    return url.split(\"?\")[0].split(\"#\")[0];\r\n  }\r\n}\r\n"],"file":"CrawlerRepository.js"}