{"version":3,"sources":["../../src/Repository/CrawlerRepository.js"],"names":["UrlParser","axios","Progress","Url","Args","Option","SqliteCrawlStatesRepository","puppeteer","CrawlerRepository","constructor","args","option","initialUrl","domain","findAllUrls","progress","crawlStatesRepository","urlsPoolSize","addPoolUrl","browser","launch","ignoreHTTPSErrors","headless","page","newPage","setJavaScriptEnabled","Promise","resolve","crawlNextUrl","processPage","url","goto","waitForRender","index","waitFor","response","content","mainFrame","headers","html","close","cleanUrl","popPoolUrl","then","data","newUrl","addUrl","urlsSize","isSingle","links","$$","linkHandle","href","evaluate","link","foundUrl","isFreshUrl","catch","urls","findAttemptedUrls","isInDomain","isNotExclusion","isNotRecursive","isNotDocument","path","parse","exclusion","exclusions","startsWith","endsWith","uri","replace","split","entries","splice","length","entry","found","filter","e"],"mappings":"AACA,OAAOA,SAAP,MAAsB,KAAtB;AACA,OAAOC,KAAP,MAAkB,OAAlB;AAEA,OAAOC,QAAP,MAAqB,sBAArB;AACA,OAAOC,GAAP,MAAgB,iBAAhB;AACA,OAAOC,IAAP,MAAiB,kBAAjB;AACA,OAAOC,MAAP,MAAmB,oBAAnB;AAEA,OAAOC,2BAAP,MAAwC,kCAAxC;AACA,OAAOC,SAAP,MAAsB,WAAtB;AAEA;;;;AAGA,eAAe,MAAMC,iBAAN,CAAwB;AACnC;;;;AAIA;;;;AAIA;;;;AAIA;;;;AASA;;;AAGAC,EAAAA,WAAW,CAACC,IAAD,EAAaC,MAAb,EAA6B;AACpC,SAAKC,UAAL,GAAmB,UAASF,IAAI,CAACG,MAAO,GAAxC;AACA,SAAKH,IAAL,GAAYA,IAAZ;AACA,SAAKC,MAAL,GAAcA,MAAd;AACH;AAED;;;;;AAGA,QAAMG,WAAN,CAAkBC,QAAlB,EAA8D;AAC1D,SAAKA,QAAL,GAAgBA,QAAhB;;AACA,QAAI,KAAKC,qBAAL,CAA2BC,YAA3B,OAA8C,CAAlD,EAAqD;AACjD,WAAKD,qBAAL,CAA2BE,UAA3B,CAAsC,KAAKN,UAA3C;AACH;;AACD,SAAKO,OAAL,GAAe,MAAMZ,SAAS,CAACa,MAAV,CAAiB;AAACC,MAAAA,iBAAiB,EAAE,IAApB;AAA0BC,MAAAA,QAAQ,EAAE;AAApC,KAAjB,CAArB;AACA,SAAKC,IAAL,GAAY,MAAM,KAAKJ,OAAL,CAAaK,OAAb,EAAlB;AACA,SAAKD,IAAL,CAAUE,oBAAV,CAA+B,IAA/B;AACA,WAAO,IAAIC,OAAJ,CAAmBC,OAAO,IAAI;AACjC,WAAKA,OAAL,GAAeA,OAAf;AACA,WAAKC,YAAL;AACH,KAHM,CAAP;AAIH;AAED;;;;;;;AAKA,QAAMC,WAAN,CAAkBC,GAAlB,EAA+B;AAC3B,UAAM,KAAKP,IAAL,CAAUQ,IAAV,CAAeD,GAAf,EAAoB;AAAC,mBAAa;AAAd,KAApB,CAAN;AACA,UAAM;AAACE,MAAAA;AAAD,QAAkB,KAAKrB,MAAL,CAAYsB,KAApC;;AACA,QAAID,aAAJ,EAAmB;AACf,YAAM,KAAKT,IAAL,CAAUW,OAAV,CAAkBF,aAAlB,CAAN;AACH;;AACD,UAAMG,QAAQ,GAAG,MAAMlC,KAAK,CAAC6B,GAAD,CAA5B;AACA,UAAMM,OAAO,GAAG,MAAM,KAAKb,IAAL,CAAUc,SAAV,GAAsBD,OAAtB,EAAtB;AACA,WAAO;AACHE,MAAAA,OAAO,EAAEH,QAAQ,CAACG,OADf;AAEHC,MAAAA,IAAI,EAAEH;AAFH,KAAP;AAIH;AAED;;;;;;AAIAR,EAAAA,YAAY,GAAG;AACX,UAAMX,YAAY,GAAG,KAAKD,qBAAL,CAA2BC,YAA3B,EAArB;;AACA,QAAIA,YAAY,KAAK,CAArB,EAAwB;AACpB,WAAKE,OAAL,CAAaqB,KAAb;AACA,aAAO,KAAKb,OAAL,CAAa,KAAKX,qBAAL,CAA2BF,WAA3B,EAAb,CAAP;AACH;;AACD,QAAIgB,GAAG,GAAGtB,iBAAiB,CAACiC,QAAlB,CACN,KAAKzB,qBAAL,CAA2B0B,UAA3B,EADM,CAAV;AAGA,SAAKb,WAAL,CAAiBC,GAAjB,EAAsBa,IAAtB,CAA2B,MAAMC,IAAN,IAAc;AACrC,YAAMC,MAAM,GAAG,IAAI1C,GAAJ,CAAQ2B,GAAR,CAAf;AACA,WAAKd,qBAAL,CAA2B8B,MAA3B,CAAkCD,MAAlC;AACA,YAAME,QAAQ,GAAG,KAAK/B,qBAAL,CAA2B+B,QAA3B,EAAjB;AACA,WAAKhC,QAAL,CACI,IAAIb,QAAJ,CAAa2C,MAAb,EAAqBD,IAAI,CAACL,IAA1B,EAAgCK,IAAI,CAACN,OAArC,EAA8CS,QAA9C,EAAwD9B,YAAY,GAAG,CAAvE,CADJ;;AAGA,UAAI,KAAKP,IAAL,CAAUsC,QAAV,EAAJ,EAA0B;AACtB,aAAKrB,OAAL,CAAa,KAAKX,qBAAL,CAA2BF,WAA3B,EAAb;AACH,OAFD,MAEO;AACH,cAAMmC,KAAK,GAAG,MAAM,KAAK1B,IAAL,CAAU2B,EAAV,CAAa,GAAb,CAApB;;AACA,aAAK,IAAIC,UAAT,IAAuBF,KAAvB,EAA8B;AAC1B,gBAAMG,IAAI,GAAG,MAAM,KAAK7B,IAAL,CAAU8B,QAAV,CAAmBC,IAAI,IAAIA,IAAI,CAACF,IAAhC,EAAsCD,UAAtC,CAAnB;AACA,cAAII,QAAQ,GAAG/C,iBAAiB,CAACiC,QAAlB,CAA2BW,IAA3B,CAAf;;AACA,cAAI,KAAKI,UAAL,CAAgBD,QAAhB,CAAJ,EAA+B;AAC3B,iBAAKvC,qBAAL,CAA2BE,UAA3B,CAAsCqC,QAAtC;AACH;AACJ;;AACD,aAAK3B,YAAL;AACH;AACJ,KApBD,EAoBG6B,KApBH,CAoBS,MAAM;AACX,WAAK7B,YAAL;AACH,KAtBD;AAuBH;AAED;;;;;;AAIA4B,EAAAA,UAAU,CAAC1B,GAAD,EAAc;AACpB,UAAM4B,IAAI,GAAG,KAAK1C,qBAAL,CAA2B2C,iBAA3B,CAA6C7B,GAA7C,CAAb;AACA,WACI4B,IAAI,KAAK,CAAT,IACA,KAAKE,UAAL,CAAgB9B,GAAhB,CADA,IAEA,KAAK+B,cAAL,CAAoB/B,GAApB,CAFA,IAGAtB,iBAAiB,CAACsD,cAAlB,CAAiChC,GAAjC,CAHA,IAIAtB,iBAAiB,CAACuD,aAAlB,CAAgCjC,GAAhC,CALJ;AAOH;AAED;;;;;AAGA+B,EAAAA,cAAc,CAAC/B,GAAD,EAAuB;AACjC,QAAI;AAACkC,MAAAA;AAAD,QAAShE,SAAS,CAACiE,KAAV,CAAgBnC,GAAhB,CAAb;;AACA,SAAK,IAAIoC,SAAT,IAAsB,KAAKvD,MAAL,CAAYsB,KAAZ,CAAkBkC,UAAxC,EAAoD;AAChD,UAAIH,IAAI,CAACI,UAAL,CAAgBF,SAAhB,CAAJ,EAAgC;AAC5B,eAAO,KAAP;AACH;AACJ;;AACD,WAAO,IAAP;AACH;AAED;;;;;;AAIA,SAAOH,aAAP,CAAqBjC,GAArB,EAA2C;AACvC,WACI,CAACA,GAAG,CAACuC,QAAJ,CAAa,MAAb,CAAD,IACA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CADD,IAEA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CAFD,IAGA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CAHD,IAIA,CAACvC,GAAG,CAACuC,QAAJ,CAAa,MAAb,CALL;AAOH;AAED;;;;;;AAIA,SAAOP,cAAP,CAAsBhC,GAAtB,EAA4C;AACxC,QAAIwC,GAAG,GAAGxC,GAAG,CAACyC,OAAJ,CAAY,gBAAZ,EAA8B,EAA9B,EAAkCC,KAAlC,CAAwC,GAAxC,CAAV;AACA,UAAMC,OAAO,GAAGH,GAAG,CAACI,MAAJ,CAAW,CAAX,EAAcJ,GAAG,CAACK,MAAlB,CAAhB;;AACA,SAAK,IAAIC,KAAT,IAAkBH,OAAlB,EAA2B;AACvB,YAAMI,KAAK,GAAGJ,OAAO,CAACK,MAAR,CAAeC,CAAC,IAAIA,CAAC,KAAKH,KAA1B,EAAiCD,MAA/C;;AACA,UAAIE,KAAK,GAAG,CAAZ,EAAe;AACX,eAAO,KAAP;AACH;AACJ;;AACD,WAAO,IAAP;AACH;AAED;;;;;AAGAjB,EAAAA,UAAU,CAAC9B,GAAD,EAAuB;AAC7B,WAAOA,GAAG,CACLyC,OADE,CACM,gBADN,EACwB,EADxB,EAEFH,UAFE,CAES,OAAO,KAAK1D,IAAL,CAAUG,MAF1B,CAAP;AAGH;AAED;;;;;AAGA,SAAO4B,QAAP,CAAgBX,GAAhB,EAAqC;AACjC,WAAOA,GAAG,CAAC0C,KAAJ,CAAU,GAAV,EAAe,CAAf,EAAkBA,KAAlB,CAAwB,GAAxB,EAA6B,CAA7B,CAAP;AACH;;AAjLkC","sourcesContent":["// @flow\nimport UrlParser from \"url\";\nimport axios from 'axios';\n\nimport Progress from \"../Model/Progress.js\";\nimport Url from \"../Model/Url.js\";\nimport Args from \"../Model/Args.js\";\nimport Option from \"../Model/Option.js\";\n\nimport SqliteCrawlStatesRepository from \"./SqliteCrawlStatesRepository.js\";\nimport puppeteer from \"puppeteer\";\n\n/**\n * This crawler repository will use a domain name as a data-source and extract urls from it.\n */\nexport default class CrawlerRepository {\n    /**\n     * The initial sitemap url.\n     */\n    initialUrl: string;\n    /**\n     * Arguments passed to the app from the user.\n     */\n    args: Args;\n    /**\n     * Options loaded for the crawl.\n     */\n    option: Option;\n    /**\n     * Repository to access the crawl state.\n     */\n    crawlStatesRepository: SqliteCrawlStatesRepository;\n    progress: (Progress) => void;\n    resolve: any;\n    browser: any;\n    page: any;\n\n    /**\n     * Build a sitemap repository\n     */\n    constructor(args: Args, option: Option) {\n        this.initialUrl = `http://${args.domain}/`;\n        this.args = args;\n        this.option = option;\n    }\n\n    /**\n     * Find all the urls on a site.\n     */\n    async findAllUrls(progress: (Progress) => void): Promise<any> {\n        this.progress = progress;\n        if (this.crawlStatesRepository.urlsPoolSize() === 0) {\n            this.crawlStatesRepository.addPoolUrl(this.initialUrl);\n        }\n        this.browser = await puppeteer.launch({ignoreHTTPSErrors: true, headless: true});\n        this.page = await this.browser.newPage();\n        this.page.setJavaScriptEnabled(true);\n        return new Promise<Url[]>(resolve => {\n            this.resolve = resolve;\n            this.crawlNextUrl();\n        });\n    }\n\n    /**\n     * Goto the page and get the html contents of the page.\n     * @param url of the page.\n     * @returns {Promise<void>}\n     */\n    async processPage(url: string) {\n        await this.page.goto(url, {\"waitUntil\": \"networkidle2\"});\n        const {waitForRender} = this.option.index;\n        if (waitForRender) {\n            await this.page.waitFor(waitForRender);\n        }\n        const response = await axios(url);\n        const content = await this.page.mainFrame().content();\n        return {\n            headers: response.headers,\n            html: content\n        }\n    }\n\n    /**\n     * Gets the page, if there are more pages it will add them to the list\n     * else, just adds the urls to the urls array.\n     */\n    crawlNextUrl() {\n        const urlsPoolSize = this.crawlStatesRepository.urlsPoolSize();\n        if (urlsPoolSize === 0) {\n            this.browser.close();\n            return this.resolve(this.crawlStatesRepository.findAllUrls());\n        }\n        let url = CrawlerRepository.cleanUrl(\n            this.crawlStatesRepository.popPoolUrl()\n        );\n        this.processPage(url).then(async data => {\n            const newUrl = new Url(url);\n            this.crawlStatesRepository.addUrl(newUrl);\n            const urlsSize = this.crawlStatesRepository.urlsSize();\n            this.progress(\n                new Progress(newUrl, data.html, data.headers, urlsSize, urlsPoolSize - 1)\n            );\n            if (this.args.isSingle()) {\n                this.resolve(this.crawlStatesRepository.findAllUrls());\n            } else {\n                const links = await this.page.$$(\"a\");\n                for (let linkHandle of links) {\n                    const href = await this.page.evaluate(link => link.href, linkHandle);\n                    let foundUrl = CrawlerRepository.cleanUrl(href);\n                    if (this.isFreshUrl(foundUrl)) {\n                        this.crawlStatesRepository.addPoolUrl(foundUrl);\n                    }\n                }\n                this.crawlNextUrl();\n            }\n        }).catch(() => {\n            this.crawlNextUrl();\n        });\n    }\n\n    /**\n     * Has the url been crawled before?\n     * @returns {boolean} true if the url has not been attempted.\n     */\n    isFreshUrl(url: string) {\n        const urls = this.crawlStatesRepository.findAttemptedUrls(url);\n        return (\n            urls === 0 &&\n            this.isInDomain(url) &&\n            this.isNotExclusion(url) &&\n            CrawlerRepository.isNotRecursive(url) &&\n            CrawlerRepository.isNotDocument(url)\n        );\n    }\n\n    /**\n     * Check to see if the url should be excluded.\n     */\n    isNotExclusion(url: string): boolean {\n        let {path} = UrlParser.parse(url);\n        for (let exclusion of this.option.index.exclusions) {\n            if (path.startsWith(exclusion)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * This crawler only crawls html pages so make sure it is not something else.\n     * The next version will handle every document type.\n     */\n    static isNotDocument(url: string): boolean {\n        return (\n            !url.endsWith(\".pdf\") &&\n            !url.endsWith(\".jpg\") &&\n            !url.endsWith(\".png\") &&\n            !url.endsWith(\".gif\") &&\n            !url.endsWith(\".doc\")\n        );\n    }\n\n    /**\n     * Some sites I have crawled urls that are recursive and grow without a 404 being thrown. This\n     * method attempts to avoid those pages.\n     */\n    static isNotRecursive(url: string): boolean {\n        let uri = url.replace(/(https|http):/i, \"\").split(\"/\");\n        const entries = uri.splice(3, uri.length);\n        for (let entry of entries) {\n            const found = entries.filter(e => e === entry).length;\n            if (found > 1) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * The index will only crawl urls on the given domain.\n     */\n    isInDomain(url: string): boolean {\n        return url\n            .replace(/(https|http):/i, \"\")\n            .startsWith(\"//\" + this.args.domain);\n    }\n\n    /**\n     * Remove url params and hashes. They can lead to recursion.\n     */\n    static cleanUrl(url: string): string {\n        return url.split(\"?\")[0].split(\"#\")[0];\n    }\n}\n"],"file":"CrawlerRepository.js"}