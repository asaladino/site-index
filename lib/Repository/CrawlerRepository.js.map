{"version":3,"sources":["../../src/Repository/CrawlerRepository.js"],"names":["CrawlerRepository","constructor","args","option","initialUrl","domain","findAllUrls","progress","crawlStatesRepository","urlsPoolSize","addPoolUrl","browser","puppeteer","launch","page","newPage","setJavaScriptEnabled","Promise","resolve","crawlNextUrl","processPage","url","goto","waitFor","index","waitForRender","mainFrame","content","close","cleanUrl","popPoolUrl","then","innerHTML","newUrl","Url","addUrl","urlsSize","Progress","isSingle","links","$$","linkHandle","href","evaluate","link","foundUrl","isFreshUrl","catch","urls","findAttemptedUrls","isInDomain","isNotExclusion","isNotRecursive","isNotDocument","path","UrlParser","parse","urlsParsed","exclusion","exclusions","startsWith","endsWith","uri","replace","split","entries","splice","length","entry","found","filter","e"],"mappings":"6FACA,gDAEA,mEACA,yDACA,2DACA,+DAEA,kGACA,4D,4oBAEA;;GAGe,KAAMA,CAAAA,iBAAkB,CACnC;;OADmC,CAKnC;;OALmC,CASnC;;OATmC,CAanC;;OAbmC,CAsBnC;;OAGAC,WAAW,CAACC,IAAD,CAAaC,MAAb,CAA6B,CACpC,KAAKC,UAAL,kBAA4BF,IAAI,CAACG,MAAjC,MACA,KAAKH,IAAL,CAAYA,IAAZ,CACA,KAAKC,MAAL,CAAcA,MACjB,CAED;;OAGMG,WAAN,CAAkBC,QAAlB,CAA8D,qDAC1D,KAAI,CAACA,QAAL,CAAgBA,QAAhB,CACA,GAAI,KAAI,CAACC,qBAAL,CAA2BC,YAA3B,KAA8C,CAAlD,CAAqD,CACjD,KAAI,CAACD,qBAAL,CAA2BE,UAA3B,CAAsC,KAAI,CAACN,UAA3C,CACH,CACD,KAAI,CAACO,OAAL,MAAqBC,oBAAUC,MAAV,EAArB,CACA,KAAI,CAACC,IAAL,MAAkB,CAAA,KAAI,CAACH,OAAL,CAAaI,OAAb,EAAlB,CACA,KAAI,CAACD,IAAL,CAAUE,oBAAV,CAA+B,IAA/B,EACA,MAAO,IAAIC,CAAAA,OAAJ,CAAmBC,OAAO,EAAI,CACjC,KAAI,CAACA,OAAL,CAAeA,OAAf,CACA,KAAI,CAACC,YAAL,EACH,CAHM,CARmD,IAY7D,CAED;;;;OAKMC,WAAN,CAAkBC,GAAlB,CAA+B,sDAC3B,KAAM,CAAA,MAAI,CAACP,IAAL,CAAUQ,IAAV,CAAeD,GAAf,CAAoB,CAAC,YAAa,cAAd,CAApB,CAAN,CACA,KAAM,CAAA,MAAI,CAACP,IAAL,CAAUS,OAAV,CAAkB,MAAI,CAACpB,MAAL,CAAYqB,KAAZ,CAAkBC,aAApC,CAAN,CACA,YAAa,CAAA,MAAI,CAACX,IAAL,CAAUY,SAAV,GAAsBC,OAAtB,EAHc,IAI9B,CAED;;;OAIAR,YAAY,EAAG,iBACX,KAAMV,CAAAA,YAAY,CAAG,KAAKD,qBAAL,CAA2BC,YAA3B,EAArB,CACA,GAAIA,YAAY,GAAK,CAArB,CAAwB,CACpB,KAAKE,OAAL,CAAaiB,KAAb,GACA,MAAO,MAAKV,OAAL,CAAa,KAAKV,qBAAL,CAA2BF,WAA3B,EAAb,CACV,CACD,GAAIe,CAAAA,GAAG,CAAGrB,iBAAiB,CAAC6B,QAAlB,CACN,KAAKrB,qBAAL,CAA2BsB,UAA3B,EADM,CAAV,CAGA,KAAKV,WAAL,CAAiBC,GAAjB,EAAsBU,IAAtB,oDAA2B,UAAMC,SAAN,CAAmB,CAC1C,KAAMC,CAAAA,MAAM,CAAG,GAAIC,aAAJ,CAAQb,GAAR,CAAf,CACA,MAAI,CAACb,qBAAL,CAA2B2B,MAA3B,CAAkCF,MAAlC,EACA,KAAMG,CAAAA,QAAQ,CAAG,MAAI,CAAC5B,qBAAL,CAA2B4B,QAA3B,EAAjB,CACA,MAAI,CAAC7B,QAAL,CACI,GAAI8B,kBAAJ,CAAaJ,MAAb,CAAqBD,SAArB,CAAgCI,QAAhC,CAA0C3B,YAAY,CAAG,CAAzD,CADJ,EAGA,GAAI,MAAI,CAACP,IAAL,CAAUoC,QAAV,EAAJ,CAA0B,CACtB,MAAI,CAACpB,OAAL,CAAa,MAAI,CAACV,qBAAL,CAA2BF,WAA3B,EAAb,CACH,CAFD,IAEO,CACH,KAAMiC,CAAAA,KAAK,MAAS,CAAA,MAAI,CAACzB,IAAL,CAAU0B,EAAV,CAAa,GAAb,CAApB,CACA,IAAK,GAAIC,CAAAA,UAAT,GAAuBF,CAAAA,KAAvB,CAA8B,CAC1B,KAAMG,CAAAA,IAAI,MAAS,CAAA,MAAI,CAAC5B,IAAL,CAAU6B,QAAV,CAAmBC,IAAI,EAAIA,IAAI,CAACF,IAAhC,CAAsCD,UAAtC,CAAnB,CACA,GAAII,CAAAA,QAAQ,CAAG7C,iBAAiB,CAAC6B,QAAlB,CAA2Ba,IAA3B,CAAf,CACA,GAAI,MAAI,CAACI,UAAL,CAAgBD,QAAhB,CAAJ,CAA+B,CAC3B,MAAI,CAACrC,qBAAL,CAA2BE,UAA3B,CAAsCmC,QAAtC,CACH,CACJ,CACD,MAAI,CAAC1B,YAAL,EACH,CACJ,CApBD,6DAoBG4B,KApBH,CAoBS,IAAM,CACX,KAAK5B,YAAL,EACH,CAtBD,CAuBH,CAED;;;OAIA2B,UAAU,CAACzB,GAAD,CAAc,CACpB,KAAM2B,CAAAA,IAAI,CAAG,KAAKxC,qBAAL,CAA2ByC,iBAA3B,CAA6C5B,GAA7C,CAAb,CACA,MACI2B,CAAAA,IAAI,GAAK,CAAT,EACA,KAAKE,UAAL,CAAgB7B,GAAhB,CADA,EAEA,KAAK8B,cAAL,CAAoB9B,GAApB,CAFA,EAGArB,iBAAiB,CAACoD,cAAlB,CAAiC/B,GAAjC,CAHA,EAIArB,iBAAiB,CAACqD,aAAlB,CAAgChC,GAAhC,CAEP,CAED;;OAGA8B,cAAc,CAAC9B,GAAD,CAAuB,CACjC,GAAIiC,CAAAA,IAAY,CAAIC,aAAUC,KAAV,CAAgBnC,GAAhB,CAAD,CAA4BoC,UAA/C,CACA,IAAK,GAAIC,CAAAA,SAAT,GAAsB,MAAKvD,MAAL,CAAYqB,KAAZ,CAAkBmC,UAAxC,CAAoD,CAChD,GAAIL,IAAI,CAACM,UAAL,CAAgBF,SAAhB,CAAJ,CAAgC,CAC5B,MAAO,MACV,CACJ,CACD,MAAO,KACV,CAED;;;OAIA,MAAOL,CAAAA,aAAP,CAAqBhC,GAArB,CAA2C,CACvC,MACI,CAACA,GAAG,CAACwC,QAAJ,CAAa,MAAb,CAAD,EACA,CAACxC,GAAG,CAACwC,QAAJ,CAAa,MAAb,CADD,EAEA,CAACxC,GAAG,CAACwC,QAAJ,CAAa,MAAb,CAFD,EAGA,CAACxC,GAAG,CAACwC,QAAJ,CAAa,MAAb,CAHD,EAIA,CAACxC,GAAG,CAACwC,QAAJ,CAAa,MAAb,CAER,CAED;;;OAIA,MAAOT,CAAAA,cAAP,CAAsB/B,GAAtB,CAA4C,CACxC,GAAIyC,CAAAA,GAAG,CAAGzC,GAAG,CAAC0C,OAAJ,CAAY,gBAAZ,CAA8B,EAA9B,EAAkCC,KAAlC,CAAwC,GAAxC,CAAV,CACA,KAAMC,CAAAA,OAAO,CAAGH,GAAG,CAACI,MAAJ,CAAW,CAAX,CAAcJ,GAAG,CAACK,MAAlB,CAAhB,CACA,IAAK,GAAIC,CAAAA,KAAT,GAAkBH,CAAAA,OAAlB,CAA2B,CACvB,KAAMI,CAAAA,KAAK,CAAGJ,OAAO,CAACK,MAAR,CAAeC,CAAC,EAAIA,CAAC,GAAKH,KAA1B,EAAiCD,MAA/C,CACA,GAAIE,KAAK,CAAG,CAAZ,CAAe,CACX,MAAO,MACV,CACJ,CACD,MAAO,KACV,CAED;;OAGAnB,UAAU,CAAC7B,GAAD,CAAuB,CAC7B,MAAOA,CAAAA,GAAG,CACL0C,OADE,CACM,gBADN,CACwB,EADxB,EAEFH,UAFE,CAES,KAAO,KAAK1D,IAAL,CAAUG,MAF1B,CAGV,CAED;;OAGA,MAAOwB,CAAAA,QAAP,CAAgBR,GAAhB,CAAqC,CACjC,MAAOA,CAAAA,GAAG,CAAC2C,KAAJ,CAAU,GAAV,EAAe,CAAf,EAAkBA,KAAlB,CAAwB,GAAxB,EAA6B,CAA7B,CACV,CAzKkC,C","sourcesContent":["// @flow\r\nimport UrlParser from \"url\";\r\n\r\nimport Progress from \"../Model/Progress\";\r\nimport Url from \"../Model/Url\";\r\nimport Args from \"../Model/Args\";\r\nimport Option from \"../Model/Option\";\r\n\r\nimport SqliteCrawlStatesRepository from \"./SqliteCrawlStatesRepository\";\r\nimport puppeteer from \"puppeteer\";\r\n\r\n/**\r\n * This crawler repository will use a domain name as a data-source and extract urls from it.\r\n */\r\nexport default class CrawlerRepository {\r\n    /**\r\n     * The initial sitemap url.\r\n     */\r\n    initialUrl: string;\r\n    /**\r\n     * Arguments passed to the app from the user.\r\n     */\r\n    args: Args;\r\n    /**\r\n     * Options loaded for the crawl.\r\n     */\r\n    option: Option;\r\n    /**\r\n     * Repository to access the crawl state.\r\n     */\r\n    crawlStatesRepository: SqliteCrawlStatesRepository;\r\n    progress: (Progress) => void;\r\n    resolve: any;\r\n    browser: any;\r\n    page: any;\r\n\r\n    /**\r\n     * Build a sitemap repository\r\n     */\r\n    constructor(args: Args, option: Option) {\r\n        this.initialUrl = `http://${args.domain}/`;\r\n        this.args = args;\r\n        this.option = option;\r\n    }\r\n\r\n    /**\r\n     * Find all the urls on a site.\r\n     */\r\n    async findAllUrls(progress: (Progress) => void): Promise<any> {\r\n        this.progress = progress;\r\n        if (this.crawlStatesRepository.urlsPoolSize() === 0) {\r\n            this.crawlStatesRepository.addPoolUrl(this.initialUrl);\r\n        }\r\n        this.browser = await puppeteer.launch();\r\n        this.page = await this.browser.newPage();\r\n        this.page.setJavaScriptEnabled(true);\r\n        return new Promise<Url[]>(resolve => {\r\n            this.resolve = resolve;\r\n            this.crawlNextUrl();\r\n        });\r\n    }\r\n\r\n    /**\r\n     * Goto the page and get the html contents of the page.\r\n     * @param url of the page.\r\n     * @returns {Promise<void>}\r\n     */\r\n    async processPage(url: string) {\r\n        await this.page.goto(url, {\"waitUntil\": \"networkidle2\"});\r\n        await this.page.waitFor(this.option.index.waitForRender);\r\n        return await this.page.mainFrame().content();\r\n    }\r\n\r\n    /**\r\n     * Gets the page, if there are more pages it will add them to the list\r\n     * else, just adds the urls to the urls array.\r\n     */\r\n    crawlNextUrl() {\r\n        const urlsPoolSize = this.crawlStatesRepository.urlsPoolSize();\r\n        if (urlsPoolSize === 0) {\r\n            this.browser.close();\r\n            return this.resolve(this.crawlStatesRepository.findAllUrls());\r\n        }\r\n        let url = CrawlerRepository.cleanUrl(\r\n            this.crawlStatesRepository.popPoolUrl()\r\n        );\r\n        this.processPage(url).then(async innerHTML => {\r\n            const newUrl = new Url(url);\r\n            this.crawlStatesRepository.addUrl(newUrl);\r\n            const urlsSize = this.crawlStatesRepository.urlsSize();\r\n            this.progress(\r\n                new Progress(newUrl, innerHTML, urlsSize, urlsPoolSize - 1)\r\n            );\r\n            if (this.args.isSingle()) {\r\n                this.resolve(this.crawlStatesRepository.findAllUrls());\r\n            } else {\r\n                const links = await this.page.$$(\"a\");\r\n                for (let linkHandle of links) {\r\n                    const href = await this.page.evaluate(link => link.href, linkHandle);\r\n                    let foundUrl = CrawlerRepository.cleanUrl(href);\r\n                    if (this.isFreshUrl(foundUrl)) {\r\n                        this.crawlStatesRepository.addPoolUrl(foundUrl);\r\n                    }\r\n                }\r\n                this.crawlNextUrl();\r\n            }\r\n        }).catch(() => {\r\n            this.crawlNextUrl();\r\n        });\r\n    }\r\n\r\n    /**\r\n     * Has the url been crawled before?\r\n     * @returns {boolean} true if the url has not been attempted.\r\n     */\r\n    isFreshUrl(url: string) {\r\n        const urls = this.crawlStatesRepository.findAttemptedUrls(url);\r\n        return (\r\n            urls === 0 &&\r\n            this.isInDomain(url) &&\r\n            this.isNotExclusion(url) &&\r\n            CrawlerRepository.isNotRecursive(url) &&\r\n            CrawlerRepository.isNotDocument(url)\r\n        );\r\n    }\r\n\r\n    /**\r\n     * Check to see if the url should be excluded.\r\n     */\r\n    isNotExclusion(url: string): boolean {\r\n        let path: string = (UrlParser.parse(url): any).urlsParsed;\r\n        for (let exclusion of this.option.index.exclusions) {\r\n            if (path.startsWith(exclusion)) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n\r\n    /**\r\n     * This crawler only crawls html pages so make sure it is not something else.\r\n     * The next version will handle every document type.\r\n     */\r\n    static isNotDocument(url: string): boolean {\r\n        return (\r\n            !url.endsWith(\".pdf\") &&\r\n            !url.endsWith(\".jpg\") &&\r\n            !url.endsWith(\".png\") &&\r\n            !url.endsWith(\".gif\") &&\r\n            !url.endsWith(\".doc\")\r\n        );\r\n    }\r\n\r\n    /**\r\n     * Some sites I have crawled urls that are recursive and grow without a 404 being thrown. This\r\n     * method attempts to avoid those pages.\r\n     */\r\n    static isNotRecursive(url: string): boolean {\r\n        let uri = url.replace(/(https|http):/i, \"\").split(\"/\");\r\n        const entries = uri.splice(3, uri.length);\r\n        for (let entry of entries) {\r\n            const found = entries.filter(e => e === entry).length;\r\n            if (found > 1) {\r\n                return false;\r\n            }\r\n        }\r\n        return true;\r\n    }\r\n\r\n    /**\r\n     * The index will only crawl urls on the given domain.\r\n     */\r\n    isInDomain(url: string): boolean {\r\n        return url\r\n            .replace(/(https|http):/i, \"\")\r\n            .startsWith(\"//\" + this.args.domain);\r\n    }\r\n\r\n    /**\r\n     * Remove url params and hashes. They can lead to recursion.\r\n     */\r\n    static cleanUrl(url: string): string {\r\n        return url.split(\"?\")[0].split(\"#\")[0];\r\n    }\r\n}\r\n"],"file":"CrawlerRepository.js"}