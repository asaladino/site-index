{"version":3,"sources":["../../src/Repository/CrawlerRepository.js"],"names":["CrawlerRepository","constructor","args","option","projectPath","initialUrl","index","domain","screenshotsPath","getScreenshotsPath","findAllUrls","progress","crawlStatesRepository","urlsPoolSize","addPoolUrl","browser","puppeteer","launch","ignoreHTTPSErrors","headless","page","newPage","setJavaScriptEnabled","credentials","login","Promise","resolve","crawlNextUrl","loginUrl","usernameSelector","passwordSelector","username","password","buttonSelector","goto","waitFor","$eval","node","value","submitElement","$","click","processPage","url","waitForRender","response","contentType","axios","get","headers","e","indexOf","frame","mainFrame","content","html","urlsSize","limit","close","cleanUrl","popPoolUrl","then","data","newUrl","Url","addUrl","Progress","screenshots","screenshot","path","join","name","isSingle","links","$$","linkHandle","href","evaluate","link","foundUrl","isFreshUrl","catch","urls","findAttemptedUrls","isInDomain","isNotExclusion","isInclusion","isNotRecursive","isNotDocument","UrlParser","parse","exclusion","exclusions","startsWith","inclusions","inclusion","endsWith","uri","replace","split","entries","splice","length","entry","found","filter"],"mappings":";;;;;;;AACA;;AACA;;AACA;;AAEA;;AACA;;AACA;;AACA;;AAEA;;AACA;;AACA;;;;;;;;AAEA;;;AAGe,MAAMA,iBAAN,CAAwB;AACnC;;;;AAIA;;;;AAIA;;;;AAIA;;;;AAUA;;;AAGAC,EAAAA,WAAW,CAACC,IAAD,EAAaC,MAAb,EAA6BC,WAA7B,EAAkD;AAAA,UAClDC,UADkD,GACpCF,MAAM,CAACG,KAD6B,CAClDD,UADkD;AAEzD,SAAKA,UAAL,GAAkBA,UAAU,GAAGA,UAAH,oBAA0BH,IAAI,CAACK,MAA/B,MAA5B;AACA,SAAKL,IAAL,GAAYA,IAAZ;AACA,SAAKC,MAAL,GAAcA,MAAd;AACA,SAAKK,eAAL,GAAuBR,iBAAiB,CAACS,kBAAlB,CAAqCL,WAArC,CAAvB;AACH;AAED;;;;;AAGMM,EAAAA,WAAN,CAAkBC,QAAlB,EAA8D;AAAA;;AAAA;AAC1D,MAAA,KAAI,CAACA,QAAL,GAAgBA,QAAhB;;AACA,UAAI,KAAI,CAACC,qBAAL,CAA2BC,YAA3B,OAA8C,CAAlD,EAAqD;AACjD,QAAA,KAAI,CAACD,qBAAL,CAA2BE,UAA3B,CAAsC,KAAI,CAACT,UAA3C;AACH;;AACD,MAAA,KAAI,CAACU,OAAL,SAAqBC,mBAAUC,MAAV,CAAiB;AAACC,QAAAA,iBAAiB,EAAE,IAApB;AAA0BC,QAAAA,QAAQ,EAAE;AAApC,OAAjB,CAArB;AACA,MAAA,KAAI,CAACC,IAAL,SAAkB,KAAI,CAACL,OAAL,CAAaM,OAAb,EAAlB;;AACA,MAAA,KAAI,CAACD,IAAL,CAAUE,oBAAV,CAA+B,IAA/B;;AACA,UAAI,KAAI,CAACnB,MAAL,CAAYoB,WAAhB,EAA6B;AACzB,cAAM,KAAI,CAACC,KAAL,EAAN;AACH;;AACD,aAAO,IAAIC,OAAJ,CAAmBC,OAAO,IAAI;AACjC,QAAA,KAAI,CAACA,OAAL,GAAeA,OAAf;;AACA,QAAA,KAAI,CAACC,YAAL;AACH,OAHM,CAAP;AAX0D;AAe7D;AAED;;;;;;AAIMH,EAAAA,KAAN,GAAc;AAAA;;AAAA;AAAA,oCACiF,MAAI,CAACrB,MAAL,CAAYoB,WAD7F;AAAA,YACHK,QADG,yBACHA,QADG;AAAA,YACOC,gBADP,yBACOA,gBADP;AAAA,YACyBC,gBADzB,yBACyBA,gBADzB;AAAA,YAC2CC,QAD3C,yBAC2CA,QAD3C;AAAA,YACqDC,QADrD,yBACqDA,QADrD;AAAA,YAC+DC,cAD/D,yBAC+DA,cAD/D;AAEV,YAAM,MAAI,CAACb,IAAL,CAAUc,IAAV,CAAeN,QAAf,CAAN;AACA,YAAM,MAAI,CAACR,IAAL,CAAUe,OAAV,CAAkB,IAAlB,CAAN;AACA,YAAM,MAAI,CAACf,IAAL,CAAUgB,KAAV,CAAgBP,gBAAhB,EAAkC,CAACQ,IAAD,EAAOnC,IAAP,KAAgBmC,IAAI,CAACC,KAAL,GAAapC,IAAI,CAAC6B,QAApE,EAA8E;AAACA,QAAAA,QAAQ,EAAEA;AAAX,OAA9E,CAAN;AACA,YAAM,MAAI,CAACX,IAAL,CAAUgB,KAAV,CAAgBN,gBAAhB,EAAkC,CAACO,IAAD,EAAOnC,IAAP,KAAgBmC,IAAI,CAACC,KAAL,GAAapC,IAAI,CAAC8B,QAApE,EAA8E;AAACA,QAAAA,QAAQ,EAAEA;AAAX,OAA9E,CAAN;AAEA,YAAMO,aAAa,SAAS,MAAI,CAACnB,IAAL,CAAUoB,CAAV,CAAYP,cAAZ,CAA5B;AACA,YAAMM,aAAa,CAACE,KAAd,EAAN;AACA,YAAM,MAAI,CAACrB,IAAL,CAAUe,OAAV,CAAkB,IAAlB,CAAN;AATU;AAUb;AAED;;;;;;;AAKMO,EAAAA,WAAN,CAAkBC,GAAlB,EAA+B;AAAA;;AAAA;AAC3B,YAAM,MAAI,CAACvB,IAAL,CAAUc,IAAV,CAAeS,GAAf,EAAoB;AAAC,qBAAa;AAAd,OAApB,CAAN;AAD2B,YAEpBC,aAFoB,GAEH,MAAI,CAACzC,MAAL,CAAYG,KAFT,CAEpBsC,aAFoB;;AAG3B,UAAIA,aAAJ,EAAmB;AACf,cAAM,MAAI,CAACxB,IAAL,CAAUe,OAAV,CAAkBS,aAAlB,CAAN;AACH;;AACD,UAAIC,QAAJ;AACA,UAAIC,WAAJ;;AACA,UAAI;AACAD,QAAAA,QAAQ,SAASE,eAAMC,GAAN,CAAUL,GAAV,CAAjB;AACAG,QAAAA,WAAW,GAAGD,QAAQ,CAACI,OAAT,CAAiB,cAAjB,CAAd;AACH,OAHD,CAGE,OAAOC,CAAP,EAAU,CACX;;AACD,UAAI,CAACJ,WAAD,IAAiBA,WAAW,IAAIA,WAAW,CAACK,OAAZ,CAAoB,WAApB,IAAmC,CAAC,CAAxE,EAA4E;AACxE,cAAMC,KAAK,SAAS,MAAI,CAAChC,IAAL,CAAUiC,SAAV,EAApB;AACA,cAAMC,OAAO,SAASF,KAAK,CAACE,OAAN,EAAtB;AACA,eAAO;AACHL,UAAAA,OAAO,EAAEJ,QAAQ,GAAGA,QAAQ,CAACI,OAAZ,GAAsB,EADpC;AAEHM,UAAAA,IAAI,EAAED;AAFH,SAAP;AAIH;;AACD,aAAO,IAAP;AArB2B;AAsB9B;AAED;;;;;;AAIA3B,EAAAA,YAAY,GAAG;AAAA;;AACX,UAAMd,YAAY,GAAG,KAAKD,qBAAL,CAA2BC,YAA3B,EAArB;AACA,UAAM2C,QAAQ,GAAG,KAAK5C,qBAAL,CAA2B4C,QAA3B,EAAjB;;AACA,QAAI3C,YAAY,KAAK,CAAjB,IAAuB2C,QAAQ,IAAI,KAAKtD,IAAL,CAAUuD,KAAtB,IAA+B,KAAKvD,IAAL,CAAUuD,KAAV,KAAoB,CAAC,CAA/E,EAAmF;AAC/E,WAAK1C,OAAL,CAAa2C,KAAb;AACA,aAAO,KAAKhC,OAAL,CAAa,KAAKd,qBAAL,CAA2BF,WAA3B,EAAb,CAAP;AACH;;AACD,QAAIiC,GAAG,GAAG3C,iBAAiB,CAAC2D,QAAlB,CACN,KAAK/C,qBAAL,CAA2BgD,UAA3B,EADM,CAAV;AAGA,SAAKlB,WAAL,CAAiBC,GAAjB,EAAsBkB,IAAtB;AAAA;AAAA;AAAA,mCAA2B,WAAMC,IAAN,EAAc;AACrC,cAAMC,MAAM,GAAG,IAAIC,YAAJ,CAAQrB,GAAR,CAAf;;AACA,YAAIoB,MAAJ,EAAY;AACR,UAAA,MAAI,CAACnD,qBAAL,CAA2BqD,MAA3B,CAAkCF,MAAlC;;AACA,gBAAMP,QAAQ,GAAG,MAAI,CAAC5C,qBAAL,CAA2B4C,QAA3B,EAAjB;;AACA,UAAA,MAAI,CAAC7C,QAAL,CACI,IAAIuD,iBAAJ,CAAaH,MAAb,EAAqBD,IAAI,CAACP,IAA1B,EAAgCO,IAAI,CAACb,OAArC,EAA8CO,QAA9C,EAAwD3C,YAAY,GAAG,CAAvE,CADJ;;AAIA,cAAK2C,QAAQ,IAAI,MAAI,CAACtD,IAAL,CAAUiE,WAAtB,IAAqC,MAAI,CAACjE,IAAL,CAAUiE,WAAV,KAA0B,CAAC,CAAjE,IAAuE,MAAI,CAACjE,IAAL,CAAUiE,WAAV,KAA0B,CAAC,CAAtG,EAAyG;AACrG,YAAA,MAAI,CAAC/C,IAAL,CAAUgD,UAAV,CAAqB;AAACC,cAAAA,IAAI,EAAEA,cAAKC,IAAL,CAAU,MAAI,CAAC9D,eAAf,YAAmCuD,MAAM,CAACQ,IAA1C;AAAP,aAArB;AACH;;AAED,cAAI,MAAI,CAACrE,IAAL,CAAUsE,QAAV,EAAJ,EAA0B;AACtB,YAAA,MAAI,CAAC9C,OAAL,CAAa,MAAI,CAACd,qBAAL,CAA2BF,WAA3B,EAAb;AACH,WAFD,MAEO;AACH,kBAAM+D,KAAK,SAAS,MAAI,CAACrD,IAAL,CAAUsD,EAAV,CAAa,GAAb,CAApB;;AACA,iBAAK,IAAIC,UAAT,IAAuBF,KAAvB,EAA8B;AAC1B,oBAAMG,IAAI,SAAS,MAAI,CAACxD,IAAL,CAAUyD,QAAV,CAAmBC,IAAI,IAAIA,IAAI,CAACF,IAAhC,EAAsCD,UAAtC,CAAnB;AACA,kBAAII,QAAQ,GAAG/E,iBAAiB,CAAC2D,QAAlB,CAA2BiB,IAA3B,CAAf;;AACA,kBAAI,MAAI,CAACI,UAAL,CAAgBD,QAAhB,CAAJ,EAA+B;AAC3B,gBAAA,MAAI,CAACnE,qBAAL,CAA2BE,UAA3B,CAAsCiE,QAAtC;AACH;AACJ;;AACD,YAAA,MAAI,CAACpD,YAAL;AACH;AACJ,SAxBD,MAwBO;AACH,UAAA,MAAI,CAACA,YAAL;AACH;AACJ,OA7BD;;AAAA;AAAA;AAAA;AAAA,SA6BGsD,KA7BH,CA6BS,MAAM;AACX,WAAKtD,YAAL;AACH,KA/BD;AAgCH;AAED;;;;;;;AAKA,SAAOlB,kBAAP,CAA0BL,WAA1B,EAA+C;AAC3C,QAAII,eAAe,GAAG6D,cAAKC,IAAL,CAAUlE,WAAV,EAAuB,aAAvB,CAAtB;;AACA,QAAI,CAAC,oBAAWI,eAAX,CAAL,EAAkC;AAC9B,yBAAUA,eAAV;AACH;;AACD,WAAOA,eAAP;AACH;AAED;;;;;;AAIAwE,EAAAA,UAAU,CAACrC,GAAD,EAAc;AACpB,UAAMuC,IAAI,GAAG,KAAKtE,qBAAL,CAA2BuE,iBAA3B,CAA6CxC,GAA7C,CAAb;AACA,WACIuC,IAAI,KAAK,CAAT,IACA,KAAKE,UAAL,CAAgBzC,GAAhB,CADA,IAEA,KAAK0C,cAAL,CAAoB1C,GAApB,CAFA,IAGA,KAAK2C,WAAL,CAAiB3C,GAAjB,CAHA,IAIA3C,iBAAiB,CAACuF,cAAlB,CAAiC5C,GAAjC,CAJA,IAKA3C,iBAAiB,CAACwF,aAAlB,CAAgC7C,GAAhC,CANJ;AAQH;AAED;;;;;AAGA0C,EAAAA,cAAc,CAAC1C,GAAD,EAAuB;AAAA,2BACpB8C,aAAUC,KAAV,CAAgB/C,GAAhB,CADoB;AAAA,QAC5B0B,IAD4B,oBAC5BA,IAD4B;;AAEjC,SAAK,IAAIsB,SAAT,IAAsB,KAAKxF,MAAL,CAAYG,KAAZ,CAAkBsF,UAAxC,EAAoD;AAChD,UAAIvB,IAAI,CAACwB,UAAL,CAAgBF,SAAhB,CAAJ,EAAgC;AAC5B,eAAO,KAAP;AACH;AACJ;;AACD,WAAO,IAAP;AACH;AAED;;;;;;AAIAL,EAAAA,WAAW,CAAC3C,GAAD,EAAuB;AAAA,UACvBmD,UADuB,GACT,KAAK3F,MAAL,CAAYG,KADH,CACvBwF,UADuB;;AAE9B,QAAI,CAACA,UAAL,EAAiB;AACb,aAAO,IAAP;AACH;;AAJ6B,4BAKjBL,aAAUC,KAAV,CAAgB/C,GAAhB,CALiB;AAAA,QAKzB0B,IALyB,qBAKzBA,IALyB;;AAM9B,SAAK,IAAI0B,SAAT,IAAsBD,UAAtB,EAAkC;AAC9B,UAAIzB,IAAI,CAACwB,UAAL,CAAgBE,SAAhB,CAAJ,EAAgC;AAC5B,eAAO,IAAP;AACH;AACJ;;AACD,WAAO,KAAP;AACH;AAED;;;;;;AAIA,SAAOP,aAAP,CAAqB7C,GAArB,EAA2C;AACvC,WACI,CAACA,GAAG,CAACqD,QAAJ,CAAa,MAAb,CAAD,IACA,CAACrD,GAAG,CAACqD,QAAJ,CAAa,MAAb,CADD,IAEA,CAACrD,GAAG,CAACqD,QAAJ,CAAa,MAAb,CAFD,IAGA,CAACrD,GAAG,CAACqD,QAAJ,CAAa,MAAb,CAHD,IAIA,CAACrD,GAAG,CAACqD,QAAJ,CAAa,MAAb,CAJD,IAIyB;AACzB,KAACrD,GAAG,CAACqD,QAAJ,CAAa,OAAb,CALD,IAMA,CAACrD,GAAG,CAACqD,QAAJ,CAAa,MAAb,CAPL;AASH;AAED;;;;;;AAIA,SAAOT,cAAP,CAAsB5C,GAAtB,EAA4C;AACxC,QAAIsD,GAAG,GAAGtD,GAAG,CAACuD,OAAJ,CAAY,gBAAZ,EAA8B,EAA9B,EAAkCC,KAAlC,CAAwC,GAAxC,CAAV;AACA,UAAMC,OAAO,GAAGH,GAAG,CAACI,MAAJ,CAAW,CAAX,EAAcJ,GAAG,CAACK,MAAlB,CAAhB;;AACA,SAAK,IAAIC,KAAT,IAAkBH,OAAlB,EAA2B;AACvB,YAAMI,KAAK,GAAGJ,OAAO,CAACK,MAAR,CAAevD,CAAC,IAAIA,CAAC,KAAKqD,KAA1B,EAAiCD,MAA/C;;AACA,UAAIE,KAAK,GAAG,CAAZ,EAAe;AACX,eAAO,KAAP;AACH;AACJ;;AACD,WAAO,IAAP;AACH;AAED;;;;;AAGApB,EAAAA,UAAU,CAACzC,GAAD,EAAuB;AAC7B,WAAOA,GAAG,CACLuD,OADE,CACM,gBADN,EACwB,EADxB,EAEFL,UAFE,CAES,OAAO,KAAK3F,IAAL,CAAUK,MAF1B,CAAP;AAGH;AAED;;;;;AAGA,SAAOoD,QAAP,CAAgBhB,GAAhB,EAAqC;AACjC,WAAOA,GAAG,CAACwD,KAAJ,CAAU,GAAV,EAAe,CAAf,CAAP;AACH;;AA7PkC","sourcesContent":["// @flow\nimport UrlParser from \"url\";\nimport axios from 'axios';\nimport path from 'path';\n\nimport Progress from \"../Model/Progress.js\";\nimport Url from \"../Model/Url.js\";\nimport Args from \"../Model/Args.js\";\nimport Option from \"../Model/Option.js\";\n\nimport SqliteCrawlStatesRepository from \"./SqliteCrawlStatesRepository.js\";\nimport puppeteer from \"puppeteer\";\nimport {existsSync, mkdirSync} from \"fs\";\n\n/**\n * This crawler repository will use a domain name as a data-source and extract urls from it.\n */\nexport default class CrawlerRepository {\n    /**\n     * The initial sitemap url.\n     */\n    initialUrl: string;\n    /**\n     * Arguments passed to the app from the user.\n     */\n    args: Args;\n    /**\n     * Options loaded for the crawl.\n     */\n    option: Option;\n    /**\n     * Repository to access the crawl state.\n     */\n    crawlStatesRepository: SqliteCrawlStatesRepository;\n    progress: (Progress) => void;\n    resolve: any;\n    browser: any;\n    page: any;\n    screenshotsPath: string;\n\n    /**\n     * Build a sitemap repository\n     */\n    constructor(args: Args, option: Option, projectPath: string) {\n        const {initialUrl} = option.index;\n        this.initialUrl = initialUrl ? initialUrl : `http://${args.domain}/`;\n        this.args = args;\n        this.option = option;\n        this.screenshotsPath = CrawlerRepository.getScreenshotsPath(projectPath);\n    }\n\n    /**\n     * Find all the urls on a site.\n     */\n    async findAllUrls(progress: (Progress) => void): Promise<any> {\n        this.progress = progress;\n        if (this.crawlStatesRepository.urlsPoolSize() === 0) {\n            this.crawlStatesRepository.addPoolUrl(this.initialUrl);\n        }\n        this.browser = await puppeteer.launch({ignoreHTTPSErrors: true, headless: true});\n        this.page = await this.browser.newPage();\n        this.page.setJavaScriptEnabled(true);\n        if (this.option.credentials) {\n            await this.login();\n        }\n        return new Promise<Url[]>(resolve => {\n            this.resolve = resolve;\n            this.crawlNextUrl();\n        });\n    }\n\n    /**\n     * Log a user in if needed.\n     * @returns {Promise<void>}\n     */\n    async login() {\n        const {loginUrl, usernameSelector, passwordSelector, username, password, buttonSelector} = this.option.credentials;\n        await this.page.goto(loginUrl);\n        await this.page.waitFor(5000);\n        await this.page.$eval(usernameSelector, (node, args) => node.value = args.username, {username: username});\n        await this.page.$eval(passwordSelector, (node, args) => node.value = args.password, {password: password});\n\n        const submitElement = await this.page.$(buttonSelector);\n        await submitElement.click();\n        await this.page.waitFor(5000);\n    }\n\n    /**\n     * Goto the page and get the html contents of the page.\n     * @param url of the page.\n     * @returns {Promise<void>}\n     */\n    async processPage(url: string) {\n        await this.page.goto(url, {\"waitUntil\": \"networkidle2\"});\n        const {waitForRender} = this.option.index;\n        if (waitForRender) {\n            await this.page.waitFor(waitForRender);\n        }\n        let response;\n        let contentType;\n        try {\n            response = await axios.get(url);\n            contentType = response.headers['content-type'];\n        } catch (e) {\n        }\n        if (!contentType || (contentType && contentType.indexOf(\"text/html\") > -1)) {\n            const frame = await this.page.mainFrame();\n            const content = await frame.content();\n            return {\n                headers: response ? response.headers : [],\n                html: content\n            }\n        }\n        return null;\n    }\n\n    /**\n     * Gets the page, if there are more pages it will add them to the list\n     * else, just adds the urls to the urls array.\n     */\n    crawlNextUrl() {\n        const urlsPoolSize = this.crawlStatesRepository.urlsPoolSize();\n        const urlsSize = this.crawlStatesRepository.urlsSize();\n        if (urlsPoolSize === 0 || (urlsSize >= this.args.limit && this.args.limit !== -1)) {\n            this.browser.close();\n            return this.resolve(this.crawlStatesRepository.findAllUrls());\n        }\n        let url = CrawlerRepository.cleanUrl(\n            this.crawlStatesRepository.popPoolUrl()\n        );\n        this.processPage(url).then(async data => {\n            const newUrl = new Url(url);\n            if (newUrl) {\n                this.crawlStatesRepository.addUrl(newUrl);\n                const urlsSize = this.crawlStatesRepository.urlsSize();\n                this.progress(\n                    new Progress(newUrl, data.html, data.headers, urlsSize, urlsPoolSize - 1)\n                );\n\n                if ((urlsSize <= this.args.screenshots && this.args.screenshots !== -1) || this.args.screenshots === -1) {\n                    this.page.screenshot({path: path.join(this.screenshotsPath, `${newUrl.name}.png`)});\n                }\n\n                if (this.args.isSingle()) {\n                    this.resolve(this.crawlStatesRepository.findAllUrls());\n                } else {\n                    const links = await this.page.$$(\"a\");\n                    for (let linkHandle of links) {\n                        const href = await this.page.evaluate(link => link.href, linkHandle);\n                        let foundUrl = CrawlerRepository.cleanUrl(href);\n                        if (this.isFreshUrl(foundUrl)) {\n                            this.crawlStatesRepository.addPoolUrl(foundUrl);\n                        }\n                    }\n                    this.crawlNextUrl();\n                }\n            } else {\n                this.crawlNextUrl();\n            }\n        }).catch(() => {\n            this.crawlNextUrl();\n        });\n    }\n\n    /**\n     * Get the path to the screenshots directory for the project.\n     * @param projectPath as a base\n     * @returns {string} the screenshots path.\n     */\n    static getScreenshotsPath(projectPath: string) {\n        let screenshotsPath = path.join(projectPath, 'screenshots');\n        if (!existsSync(screenshotsPath)) {\n            mkdirSync(screenshotsPath);\n        }\n        return screenshotsPath;\n    }\n\n    /**\n     * Has the url been crawled before?\n     * @returns {boolean} true if the url has not been attempted.\n     */\n    isFreshUrl(url: string) {\n        const urls = this.crawlStatesRepository.findAttemptedUrls(url);\n        return (\n            urls === 0 &&\n            this.isInDomain(url) &&\n            this.isNotExclusion(url) &&\n            this.isInclusion(url) &&\n            CrawlerRepository.isNotRecursive(url) &&\n            CrawlerRepository.isNotDocument(url)\n        );\n    }\n\n    /**\n     * Check to see if the url should be excluded.\n     */\n    isNotExclusion(url: string): boolean {\n        let {path} = UrlParser.parse(url);\n        for (let exclusion of this.option.index.exclusions) {\n            if (path.startsWith(exclusion)) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * Check to see if the url should be included. If there are no inclusions,\n     * then everything is included.\n     */\n    isInclusion(url: string): boolean {\n        const {inclusions} = this.option.index;\n        if (!inclusions) {\n            return true;\n        }\n        let {path} = UrlParser.parse(url);\n        for (let inclusion of inclusions) {\n            if (path.startsWith(inclusion)) {\n                return true;\n            }\n        }\n        return false;\n    }\n\n    /**\n     * This crawler only crawls html pages so make sure it is not something else.\n     * The next version will handle every document type.\n     */\n    static isNotDocument(url: string): boolean {\n        return (\n            !url.endsWith(\".pdf\") &&\n            !url.endsWith(\".jpg\") &&\n            !url.endsWith(\".png\") &&\n            !url.endsWith(\".gif\") &&\n            !url.endsWith(\".xml\") && // content-type might be html but we get xml.\n            !url.endsWith(\".docx\") &&\n            !url.endsWith(\".doc\")\n        );\n    }\n\n    /**\n     * Some sites I have crawled urls that are recursive and grow without a 404 being thrown. This\n     * method attempts to avoid those pages.\n     */\n    static isNotRecursive(url: string): boolean {\n        let uri = url.replace(/(https|http):/i, \"\").split(\"/\");\n        const entries = uri.splice(3, uri.length);\n        for (let entry of entries) {\n            const found = entries.filter(e => e === entry).length;\n            if (found > 1) {\n                return false;\n            }\n        }\n        return true;\n    }\n\n    /**\n     * The index will only crawl urls on the given domain.\n     */\n    isInDomain(url: string): boolean {\n        return url\n            .replace(/(https|http):/i, \"\")\n            .startsWith(\"//\" + this.args.domain);\n    }\n\n    /**\n     * Remove url params and hashes. They can lead to recursion.\n     */\n    static cleanUrl(url: string): string {\n        return url.split(\"#\")[0];\n    }\n}\n"],"file":"CrawlerRepository.js"}